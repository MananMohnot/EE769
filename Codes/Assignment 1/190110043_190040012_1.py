# -*- coding: utf-8 -*-
"""190110043_190040012_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14ScOUVrcarTENPPqT-LHSAYIeFKDVA3N

#**EE769 Introduction to Machine Learning**

#Assignment 1: Gradient Descent, Linear Regression, and Regularization


**Template and Instructions**



1. Up to two people can team up, but only one should submit, and both should understand the entire code.
2. Every line of code should end in a comment explaining the line
3. It is recommended to solve the assignment in Google Colab.
Write your roll no.s separated by commas here: 190110043, 190040012
4. Write your names here: Manan Mohnot, Aman Jain
5. There are two parts to the assignment. In the Part 1, the code format has to be strictly followed to enable auto-grading. In the second part, you can be creative.
6. **You can discuss with other groups or refer to the internet without being penalized, but you cannot copy their code and modify it. Write every line of code and comment on your own.**

#**Part 1 begins ...**
**Instructions to be strictly followed:**

1. Do not add any code cells or markdown cells until the end of this part. Especially, do not change the blocks that say "TEST CASES, DO NOT CHANGE"
2. In all other cells only add code where it says "CODE HERE".
3. If you encounter any raise NotImplementedError() calls you may comment them out.

We cannot ensure correct grading if you change anything else, and you may be penalised for not following these instructions.

## Import Statements
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

"""## Normalize function 


"""

def Normalize(X): # Output should be a normalized data matrix of the same dimension
    '''
    Normalize all columns of X using mean and standard deviation
    '''
    # YOUR CODE HERE
    PI = 3.141592  #Declaring the value of Pi as constant
    mu=np.mean(X,axis=0)  # taking mean of matrix x column wise
    sigma = np.std(X,axis=0) # taking std of matrix x column wise
    X=(X-mu)/sigma # X is normalised matrix
    return X # returning X
    #raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 - 1 dimensional array'''
#X=np.array([[1,2,3],[3,4,5],[7,8,9]])
X1=np.array([1,2,3])
np.testing.assert_array_almost_equal(Normalize(X1),np.array([-1.224,  0.      ,  1.224]),decimal=3)
''' case 2 - 2 dimensional array'''
X2=np.array([[4,7,6],[3,8,9],[5,11,10]])
np.testing.assert_array_almost_equal(Normalize(X2),np.array([[ 0.  , -0.980581, -1.372813],[-1.224745, -0.392232,  0.392232],[ 1.224745,  1.372813,  0.980581]]))
''' case 3 - 1 dimensional array with float'''
X3=np.array([5.5,6.7,3.2,6.7])
np.testing.assert_array_almost_equal(Normalize(X3),np.array([-0.017,  0.822, -1.627,  0.822]),decimal=3)

"""## Prediction Function

Given X and w, compute the predicted output. Do not forget to add 1's in X
"""

def Prediction (X, w): # Output should be a prediction vector y
    '''
    Compute Prediction given an input datamatrix X and weight vecor w. Output y = [X 1]w where 1 is a vector of all 1s 
    '''
    # YOUR CODE HERE
    N=np.shape(X)[0] # finding the no of rows of X
    X1 = np.c_[X,np.ones(N)] # adding a column of ones at the end
    prediction=(X1.dot(w)) # prediction is X1 x w
    #prediction = np.sum(X1*w,axis=1)
    #print(np.shape(prediction))
    return prediction # returning prediction
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 - Known input output matrix and weights 1'''
X1 = np.array([[3,2],[1,1]])
w1 = np.array([2,1,1])
np.testing.assert_array_equal(Prediction(X1,w1),np.array([9,4]))

"""## Loss Functions

Code the four  loss functions:

1. MSE loss is only for the error
2. MAE loss is only for the error
3. L2 loss is for MSE and L2 regularization, and can call MSE loss
4. L1 loss is for MSE and L1 regularization, and can call MSE loss
"""

def MSE_Loss (X, t, w, lamda =0): # Ouput should be a single number
    '''
    lamda=0 is a default argument to prevent errors if you pass lamda to a function that doesn't need it by mistake. 
    This allows us to call all loss functions with the same input format.
    
    You are encouraged read about default arguments by yourself online if you're not familiar.
    '''
    # YOUR CODE HERE
    N=np.shape(X)[0]
    prediction = Prediction(X,w)
    #print(N)
    mse = (np.sum((t - prediction)**2))/(N)
    return mse
    
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_almost_equal(MSE_Loss(X,t,w),0.53,decimal=3)

def MAE_Loss (X, t, w, lamda = 0): # Output should be a single number
    # YOUR CODE HERE
    N=np.shape(X)[0] # finding the no of rows of X
    prediction = Prediction(X,w) # using prediction function 
    mae = (np.sum(abs(t - prediction)))/(N) # calculating mean absolue error
    return mae # returning mai
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_almost_equal(MAE_Loss(X,t,w),0.700,decimal=3)

def L2_Loss (X, t, w, lamda): # Output should be a single number based on L2-norm (with sqrt)
    ''' Need to specify what inputs are'''
    # YOUR CODE HERE
    N=np.shape(X)[0] # finding the no of rows of X
    prediction = Prediction(X,w) # calling the prediction value
    #print(N)
    sum_w=0     # initializing a variable to store the sum of weights
    for i in w[:-1]: # starting the for loop
        sum_w=sum_w + i*i # adding the weights
    L2_loss = (np.sum((t - prediction)**2))/N + lamda*np.sqrt(sum_w) # calculating the L2 Loss
    return L2_loss # returning
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_almost_equal(L2_Loss(X,t,w,0.5),1.675,decimal=3)

def L1_Loss (X, t, w, lamda): # Output should be a single number
    # YOUR CODE HERE
    N=np.shape(X)[0] # finding the no of rows of X
    prediction = Prediction(X,w) # calculating the prediction
    sum_o=0  # for storing the sum of absolute values of weights
    for i in w[:-1]: # for loop starts
        sum_o=sum_o + np.abs(i) # adding the absolute values of weights
    L1_loss = (np.sum((t - prediction)**2))/N + lamda*sum_o # L1 loss value calculation
    return L1_loss # returning 
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_almost_equal(L1_Loss(X,t,w,0.5),2.280,decimal=3)

def NRMSE_Metric (X, t, w, lamda=0): # Output should be a single number. RMSE/std_dev(t)
    # YOUR CODE HERE 
    N=np.shape(X)[0] # finding the no of rows of X
    prediction = Prediction(X,w) # calculates the prediction value
    nrmse = np.sqrt(((np.sum((t - prediction)**2))/(N)))/(np.std(t)) # calculating nrmse
    return nrmse # returning nrmse
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' Test case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_almost_equal(NRMSE_Metric(X,t,w,0.5),0.970,decimal=3)

"""## Gradient function
Each Loss function will have its own gradient function:

1. MSE gradient is only for the error
2. MAE gradient is only for the error
3. L2 gradient is for MSE and L2 regularization, and can call MSE gradient
4. L1 gradient is for MSE and L1 regularization, and can call MSE gradient
"""

def MSE_Gradient (X, t, w, lamda=0): # Output should have the same size as w
    # YOUR CODE HERE
    N=np.shape(X)[0] # finding the no of rows of X
    X1 = np.c_[X,np.ones(N)] # adding a column  of 1
    grad = (X1.T.dot(X1.dot(w)-t)) # taking the gradient, the derivative was calcualted on paper
    return grad # returning gradient
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_array_almost_equal(MSE_Gradient(X,t,w),np.array([2.55, 2.94, 2.9 , 0.4 ]),decimal=3)

def MAE_Gradient (X, t, w, lamda=0): # Output should have the same size as w
    # YOUR CODE HERE
    N=np.shape(X)[0] # finding the no of rows of X
    X1 = np.c_[X,np.ones(N)] # adding a column
    s = (X1.dot(w)-t)/(np.abs(X1.dot(w)-t)) # calculating an array keeping the absolute function in mind
    grad=X1.T.dot(s)/2 # calcualting the gradient
    # print(grad)
    return grad # returning the gradient
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_array_almost_equal(MAE_Gradient(X,t,w),np.array([0.75,  0.3 ,  0.5 , 0.]),decimal=3)

def L2_Gradient (X, t, w, lamda): # Output should have the same size as w
    # YOUR CODE HERE
    #TODO one value not matching
    N=np.shape(X)[0] # finding the no of rows of X
    sum_w=0 # variable for storing the sum
    for i in w[:-1]:
        sum_w=sum_w + i*i # summing up weights
    mse = MSE_Gradient(X,t,w,lamda=0) # calcuating the mse
    f = w*lamda/(np.sqrt(sum_w)) # the other term used in grad
    grad = mse + w*lamda/(np.sqrt(sum_w)) # calculating the grad
    grad[-1]-= f[-1] # subtracting the term corresponding to bias
    return grad # returning the grad
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_array_almost_equal(L2_Gradient(X,t,w,0.5),np.array([2.986, 2.721, 3.009 , 0.4 ]),decimal=3)

def L1_Gradient (X, t, w, lamda): # Output should have the same size as w
    # YOUR CODE HERE
    #TODO one value not matching
    mse = MSE_Gradient(X,t,w,lamda=0) # calculating the mse
    N=np.shape(X)[0] # finding the no of rows of X
    X1 = np.c_[X,np.ones(N)] # adding a column
    s = w/(abs(w)) # calculating the signs
    f = lamda*s # a term for gradient
    grad=mse + f # calculating the gradient
    grad[-1]-= f[-1] # deducting the bias term
    return grad # returning the gradient
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_array_almost_equal(L1_Gradient(X,t,w,0.5),np.array([3.05, 2.44, 3.4 , 0.4 ]),decimal=3)

"""## Gradient Descent Function

"""

def Gradient_Descent (X, X_val, t, t_val, w, lamda, max_iter, epsilon, lr, lossfunc, gradfunc): # See output format in 'return' statement
    # YOUR CODE HERE
    iter = 0 # the iteration variable
    step = 1 #  step
    loss_old = lossfunc(X,t,w,lamda) # calculating the loss
    while iter < max_iter and step > epsilon: # conditions for while loop
      w = w - lr*gradfunc(X,t,w,lamda) # updating the weights 
      loss = lossfunc(X,t,w,lamda) # calculating the loss for the new weights
      step = abs(loss - loss_old) # taking the difference of loss
      loss_old = loss # assigning the new value
      iter+=1 # increasing the iteration number
    w_final = w # a variable holding weights
    train_loss_final = lossfunc(X,t,w_final,lamda) # value of train loss for w_final
    validation_loss_final = lossfunc(X_val,t_val,w_final,lamda) # validation loss
    validation_NRMSE = NRMSE_Metric(X_val,t_val,w_final,lamda) # nrmse loss
    #raise NotImplementedError()
    return w_final, train_loss_final, validation_loss_final, validation_NRMSE #You should return variables structured like this.

'''
TEST CASES, DO NOT CHANGE
'''
X=np.array([[23,24],[1,2]])
t=np.array([4,5])
X_val=np.array([[3,4],[5,6]])
t_val=np.array([3,4])
w=np.array([3,2,1])
results =Gradient_Descent (X, X_val, t, t_val, w, 0.1, 100, 1e-10, 1e-5, L2_Loss,L2_Gradient) 
np.testing.assert_allclose([results[1]],[697.919],rtol =0.05)
np.testing.assert_allclose([results[2]],[20],atol=5) # we expect around 17.5  but some students got 24 which we will also accept
#Instructor Values of results[1] and results [2] are 697.919 and 17.512 respectively

"""## Pseudo Inverse Method

You have to implement a slightly more advanced version, with L2 penalty:

w = (X' X + lambda I)^(-1) X' t.

See, for example: Section 2 of https://web.mit.edu/zoya/www/linearRegression.pdf

Here, the column of 1's in assumed to be included in X
"""

def Pseudo_Inverse (X, t, lamda): # Output should be weight vector
    # YOUR CODE HERE
    X = np.c_[X,np.ones(np.shape(X)[0])] # adding a column for bias
    I = np.identity(np.shape(X)[1]) #Initializing Identity Matrix
    psuedo = np.dot(np.linalg.inv((np.dot(X.T,X) + lamda*I)),np.dot(X.T,t)) #calculating psuedo inverse according to the formula 
    #raise NotImplementedError()
    return psuedo #returning the psuedo inverse

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 - other data'''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
np.testing.assert_array_almost_equal(Pseudo_Inverse(X,t,0.5),np.array([ 0.491,  0.183,  0.319, -0.002]),decimal=3)

"""#... Part 1 ends Below this you be more creative. Just comment out the lines where you save files (e.g. test predictions).

#**Part 2 begins ...**

**Instructions to be loosely followed (except number 8):**

1. Add more code and text cells between this and the last cell.
2. Read training data from: https://www.ee.iitb.ac.in/~asethi/Dump/TempTrain.csv only. Do not use a local copy of the dataset.
3. Find the best lamda for **MSE+lamda*L2(w)** loss function. Plot training and validation RMSE vs. 1/lamda (1/lamda represents model complexity). Print weights, validation RMSE, validation NRMSE for the best lamda.
4. Find the best lamda for **MSE+lamda*L1(w)** loss function. Plot training and validation RMSE vs. 1/lamda (1/lamda represents model complexity). Print weights, validation RMSE, validation NRMSE for the best lamda.
5. Find the best lamda for the **pseudo-inv method**. Plot training and validation RMSE vs. 1/lamda (1/lamda represents model complexity). Print weights, validation RMSE, validation NRMSE for the best lamda.
6. Write your observations and conclusions.
7. Read test data from: https://www.ee.iitb.ac.in/~asethi/Dump/TempTest.csv only. Do not use a local copy of the dataset. Predict its dependent (missing last column) using the model with the lowest MSE, RMSE, or NRMSE. Save it as a file RollNo1_RollNo2_1.csv.
8. **Disable the prediction csv file saving statement and submit this entire .ipynb file, .py file, and .csv file as a single RollNo1_RollNo2_1.zip file.**
"""

url_train="https://www.ee.iitb.ac.in/~asethi/Dump/TempTrain.csv" #referencing url of data
df_train = pd.read_csv(url_train) #importing train data
# display(df_train) # display train data

url_test = "https://www.ee.iitb.ac.in/~asethi/Dump/TempTest.csv" #referencing url of test data
df_test = pd.read_csv(url_test) #importing test data
# display(df_test) # display test data
# print(df_train.corr())

train_set = df_train[0:int(0.8*len(df_train))] #spliiting train data into 80:20 train:validation set
val_set = df_train[int(0.8*len(df_train)):]

train_set = train_set.apply(Normalize) #Normailizing training set
val_set= val_set.apply(Normalize) #Normailizing validation set

X_train = train_set.iloc[:,:-1] # Removing predicted column from dataset
t_train = train_set.iloc[:,-1] # Making dataset with predicted column 
X_val = val_set.iloc[:,:-1] # Removing predicted column from dataset
t_val = val_set.iloc[:,-1] # Making dataset with predicted column

X_train #checking training data

def RMSE_Metric (X, t, w, lamda=0): #defining RMSE Metric 
    # YOUR CODE HERE
    N=np.shape(X)[0] # finding the no of rows of X
    prediction = Prediction(X,w) #calculates prediction value
    rmse = np.sqrt(((np.sum((t - prediction)**2))/(N))) # calculates rmse
    return rmse #returns rmse

def R2(t_val,t_pred): #defining R2 Metric
  tmean = np.mean(t_val) # Mean of Output of validation set
  r2 = 1 - np.sum((t_pred-t_val)**2)/np.sum((t_val - tmean)**2) # calculating r2
  return r2 #returns r2

np.random.seed(0) #defining seed for random numbers
w = np.random.randn(X_train.shape[1] + 1) #defining random weight vector
# w=np.ones(X_train.shape[1] + 1)
# print(w)
#  0.1, 100, 1e-10, 1e-5, L2_Loss,L2_Gradient
# w_final, train_loss_final, validation_loss_final, validation_NRMSE
lamda_list_L1= [25,33,50,75,100,250,500] #lamda list for L1
lamda_list_L2 = [250,275,300,350, 400, 500,750,1000] #lamda list for L2
lamda_list_psuedo =[500,600,750,1000,1500,2000] #lamda list for Psuedo Inverse
reciprocal_L1 = [1/x for x in lamda_list_L1] #reciprocal of lamda list for L1
reciprocal_L2 = [1/x for x in lamda_list_L2] #reciprocal of lamda list for L2
reciprocal_psuedo = [1/x for x in lamda_list_psuedo] #reciprocal of lamda list for psuedo

train_losses_L2 = [] #initializing empty set to store train losses L2
val_losses_L2 = [] #initializing empty set to store validation losses L2
train_losses_L1 = [] #initializing empty set to store train losses L1
val_losses_L1 = [] #initializing empty set to store validation losses L1
weight_L1=[] #initializing empty set to store weights L1
weight_L2=[] #initializing empty set to store weights L2
weight_psuedo=[] #initializing empty set to store weights psuedo
NRMSE_L1_val=[] #initializing empty set to store validation NRMSE L1
NRMSE_L2_val=[] #initializing empty set to store validation NRMSE L2
NRMSE_psuedo_train=[] #initializing empty set to store training NRMSE psuedo inverse
NRMSE_psuedo_val=[] #initializing empty set to store validation NRMSE psuedo inverse
RMSE_psuedo_train=[] #initializing empty set to store training RMSE psuedo inverse
RMSE_psuedo_val=[] #initializing empty set to store validation RMSE psuedo inverse
RMSE_L1_train=[] #initializing empty set to store training RMSE L1
RMSE_L1_val=[] #initializing empty set to store validation RMSE L1
RMSE_L2_train=[] #initializing empty set to store training RMSE L2
RMSE_L2_val=[] #initializing empty set to store validation RMSE L2
  # L2 model
for lamda in lamda_list_L2:
  #finding optimal w for diff lamda using grad descent
    w_final, train_loss_final, validation_loss_final, validation_NRMSE = Gradient_Descent(X_train, X_val, t_train, t_val, w, lamda, 1000, 1e-10, 1e-5, L2_Loss, L2_Gradient)
    weight_L2.append(w_final) #appending weights
    train_losses_L2.append(train_loss_final) #appending train loss
    val_losses_L2.append(validation_loss_final) #appending validation loss
    NRMSE_L2_val.append(validation_NRMSE) #appending Nrmse
    validation_RMSE_L2 = RMSE_Metric(X_val,t_val,w_final,lamda) #finding validation rmse
    training_RMSE_L2 = RMSE_Metric(X_train,t_train,w_final,lamda) #finding training rmse
    RMSE_L2_train.append(training_RMSE_L2) #appending training rmse
    RMSE_L2_val.append(validation_RMSE_L2) #appending validation rmse
    #L1 model
for lamda in lamda_list_L1:
  #finding optimal w for diff lamda using grad descent
    w_final1, train_loss_final1, validation_loss_final1, validation_NRMSE1 = Gradient_Descent(X_train, X_val, t_train, t_val, w, lamda, 1000, 1e-10, 1e-5, L1_Loss, L1_Gradient)
    weight_L1.append(w_final1) #appending weights
    train_losses_L1.append(train_loss_final1) #appending train loss
    val_losses_L1.append(validation_loss_final1) #appending validation loss
    NRMSE_L1_val.append(validation_NRMSE1) #appending Nrmse
    validation_RMSE_L1 = RMSE_Metric(X_val,t_val,w_final1,lamda) #finding validation rmse
    training_RMSE_L1 = RMSE_Metric(X_train,t_train,w_final1,lamda) #finding training rmse
    RMSE_L1_train.append(training_RMSE_L1)#appending training rmse
    RMSE_L1_val.append(validation_RMSE_L1) #appending validation rmse
    #psuedo model
for lamda in lamda_list_psuedo:
    psuedo = Pseudo_Inverse (X_train, t_train, lamda) #finding optimal w for diff lamda using psuedo inverse
    weight_psuedo.append(psuedo) #appending weights
    validation_NRMSE_ps = NRMSE_Metric(X_val,t_val,psuedo,lamda) #finding validation nrmse
    training_NRMSE_ps = NRMSE_Metric(X_train,t_train,psuedo,lamda) #finding training nrmse
    validation_RMSE_ps = RMSE_Metric(X_val,t_val,psuedo,lamda) #finding validation rmse
    training_RMSE_ps = RMSE_Metric(X_train,t_train,psuedo,lamda) #finding training rmse
    NRMSE_psuedo_train.append(training_NRMSE_ps) #appending training nrmse
    NRMSE_psuedo_val.append(validation_NRMSE_ps) #appending validation nrmse
    RMSE_psuedo_train.append(training_RMSE_ps) #appending training rmse
    RMSE_psuedo_val.append(validation_RMSE_ps) #appending validation rmse

plt.figure(figsize=(12, 15)) #defining plot size
plt.subplot(3, 2, 1) #Subplot of 3 rows and 2 columns
plt.scatter(reciprocal_L1, RMSE_L1_train) #Plot of Training RMSE of L1 vs 1/lamda
plt.title("Training RMSE for L1") #plot title
plt.xlabel('1/lamda') #X label
plt.ylabel('RMSE') #Y label
plt.subplot(3, 2, 2)
plt.scatter(reciprocal_L1, RMSE_L1_val) #Plot of Validation RMSE of L1 vs 1/lamda
plt.title("Validation RMSE for L1") #plot title
plt.xlabel('1/lamda') #X label
plt.ylabel('RMSE') #Y label
plt.subplot(3, 2, 3)
plt.scatter(reciprocal_L2, RMSE_L2_train) #Plot of Training RMSE of L2 vs 1/lamda
plt.title("Training RMSE for L2") #plot title
plt.xlabel('1/lamda') #X label
plt.ylabel('RMSE') #Y label
plt.subplot(3, 2, 4)
plt.scatter(reciprocal_L2, RMSE_L2_val) #Plot of Validation RMSE of L2 vs 1/lamda
plt.title("Validation RMSE for L2") #plot title
plt.xlabel('1/lamda') #X label
plt.ylabel('RMSE') #Y label
plt.subplot(3, 2, 5)
plt.scatter(reciprocal_psuedo, RMSE_psuedo_train) #Plot of Training RMSE of Psuedo Inverse vs 1/lamda
plt.title("Training RMSE for Psuedo Inverse") #plot title
plt.xlabel('1/lamda') #X label
plt.ylabel('RMSE') #Y label
plt.subplot(3, 2, 6)
plt.scatter(reciprocal_psuedo, RMSE_psuedo_val) #Plot of Validation RMSE of Psuedo vs 1/lamda
plt.title("Validation RMSE for Psuedo Inverse") #plot title
plt.xlabel('1/lamda') #X label
plt.ylabel('RMSE') #Y label

plt.figure(figsize=(12, 12)) #defining plot size
plt.subplot(2, 2, 1) #Subplot of 2 rows and 2 columns
plt.scatter(reciprocal_L1, NRMSE_L1_val) #Plot of Validation NRMSE of L1 vs 1/lamda
plt.title("Validation NRMSE for L1") #plot title
plt.xlabel('1/lamda') #X label
plt.ylabel('NRMSE') #Y label
plt.subplot(2, 2, 2)
plt.scatter(reciprocal_L2, NRMSE_L2_val) #Plot of Validation NRMSE of L2 vs 1/lamda
plt.title("Validation NRMSE for L2") #plot title
plt.xlabel('1/lamda') #X label
plt.ylabel('NRMSE') #Y label
plt.subplot(2, 2, 3)
plt.scatter(reciprocal_psuedo, NRMSE_psuedo_train) #Plot of Training RMSE of Psuedo vs 1/lamda
plt.title("Training NRMSE for Psuedo Inverse") #plot title
plt.xlabel('1/lamda') #X label
plt.ylabel('NRMSE') #Y label
plt.subplot(2, 2, 4)
plt.scatter(reciprocal_psuedo, NRMSE_psuedo_val) #Plot of Validation RMSE of Psuedo vs 1/lamda
plt.title("Validation NRMSE for Psuedo Inverse") #plot title
plt.xlabel('1/lamda') #X label
plt.ylabel('NRMSE') #Y label

plt.figure(figsize=(12, 12)) #defining plot size
plt.subplot(2, 2, 1) #Subplot of 2 rows and 2 columns
plt.scatter(reciprocal_L2, train_losses_L2) #Plot of Training Loss of L2 vs 1/lamda
plt.title("Training Losses for L2") #plot title
plt.xlabel('1/lamda') #X label
plt.ylabel('Loss') #Y label
plt.subplot(2, 2, 2)
plt.scatter(reciprocal_L2, val_losses_L2) #Plot of Validation Loss of L2 vs 1/lamda
plt.title("Validation Losses for L2") #plot title
plt.xlabel('1/lamda') #X label
plt.ylabel('Loss') #Y label
plt.subplot(2, 2, 3)
plt.scatter(reciprocal_L1, train_losses_L1) #Plot of Training Loss of L1 vs 1/lamda
plt.title("Training Losses for L1") #plot title
plt.xlabel('1/lamda') #X label
plt.ylabel('Loss') #Y label
plt.subplot(2, 2, 4)
plt.scatter(reciprocal_L1, val_losses_L1) #Plot of Validation Loss of L1 vs 1/lamda
plt.title("Validation Losses for L1") #plot title
plt.xlabel('1/lamda') #X label
plt.ylabel('Loss') #Y label

"""Model"""

print("For L1 \n", "Lamda:",lamda_list_L1[4],"\n","Weights",weight_L1[4],"\n","Validation RMSE:",RMSE_L1_val[4],"\n","Validation NRMSE:",NRMSE_L1_val[4],"\n")
print("For L2 \n", "Lamda:",lamda_list_L2[5],"\n","Weights",weight_L2[5],"\n","Validation RMSE:",RMSE_L2_val[5],"\n","Validation NRMSE:",NRMSE_L2_val[5],"\n")
print("For Psuedo Inverse \n", "Lamda:",lamda_list_psuedo[3],"\n","Weights",weight_psuedo[3],"\n","Validation RMSE:",RMSE_L1_val[3],"\n","Validation NRMSE:",NRMSE_L1_val[3])

#Predicted for validation data
X_val_for_pred=X_val
X_val_for_pred ['bias'] =  np.ones((1217, 1)) #adding column for bias
X_val_for_pred['Prediction'] =  [np.dot(weight_L1[4],X_val_for_pred.iloc[i,:]) for i in range(X_val_for_pred.shape[0])] #predicting for validation data

index=range(1,1218) #X axis of plot
plt.figure(figsize=(12, 12)) #Plot Size
plt.scatter(index,X_val_for_pred['Prediction'],label="Predicted") #plot of predicted values
plt.scatter(index,t_val,label='Actual') #plot of actual values
plt.title("Predicted vs. Actual plot for the validation data") #plot title
plt.legend() #shows legends

index=range(1,1218) #X axis of plot
plt.figure(figsize=(10, 6)) #Plot Size
plt.hist(X_val_for_pred['Prediction'] - t_val, bins = 30,  edgecolor="white"); #plot of predicted values
plt.grid()
plt.title("Error between validate and predicted")
plt.ylabel("Frequency")
plt.xlabel("error")

r=R2(t_val,X_val_for_pred['Prediction']) #R2 score for best model
print("R2 score for best model (L1 with lamda = 100):",r)

df_test ['bias'] =  np.ones((df_test.shape[0], 1)) #adding column for bias
df_test['Prediction'] =  [np.dot(weight_L1[4],df_test.iloc[i,:]) for i in range(df_test.shape[0])] #predicting for test data

# df_test.to_csv('prediction.csv') # saving file

"""#**... Part 2 ends.**

1. Write the name or roll no.s of friends from outside your group with whom you discussed the assignment here (no penalty for mere discussion without copying code): 
2. Write the links of sources on the internet referred here (no penalty for mere consultation without copying code): 
"""